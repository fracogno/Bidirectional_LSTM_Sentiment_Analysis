{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import dictionary\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Text processing functions\n",
    "'''\n",
    "def preprocess_sentence(sentence):\n",
    "    # Transform some punctuation to space\n",
    "    line = re.sub(r\"[,.;@#?!]+\\ *\", \" \", sentence)\n",
    "    \n",
    "    # Convert to lower case\n",
    "    line = line.lower()\n",
    "\n",
    "    # Tokenize words\n",
    "    default_wt = nltk.word_tokenize\n",
    "    line = default_wt(line)\n",
    "    \n",
    "    return line\n",
    "\n",
    "\n",
    "def max_length_sentence(dataset):\n",
    "    return max([len(line) for line in dataset])\n",
    "\n",
    "\n",
    "def pad_sentence(tokenized_sentence, max_length_sentence, padding_value=0):\n",
    "    \n",
    "    pad_length = max_length_sentence - len(tokenized_sentence)\n",
    "    sentence = list(tokenized_sentence)\n",
    "    \n",
    "    if pad_length > 0:\n",
    "        return np.pad(tokenized_sentence, (0, pad_length), mode='constant', constant_values=int(padding_value))\n",
    "    else:\n",
    "        return sentence[:max_length_sentence]\n",
    "\n",
    "\n",
    "# Dataset format: \"sentence \\t score \\n\"\n",
    "# Score is either 1 (for positive) or 0 (for negative)\n",
    "def get_data(directory):\n",
    "    X, Y = [], []\n",
    "\n",
    "    # Iterate over fils names in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "\n",
    "        if not filename.startswith('.'):            \n",
    "            with open(str(directory + '/' + filename)) as file:\n",
    "                \n",
    "                for line in file:\n",
    "                    splitted = line.split(\"\\t\")\n",
    "                    X.append(preprocess_sentence(splitted[0]))\n",
    "                    Y.append(int(splitted[1].split(\"\\n\")[0]))\n",
    "        \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "    Neural Network functions\n",
    "'''\n",
    "def new_weights(shape, name=None):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=name)\n",
    "\n",
    "\n",
    "def new_biases(length, name=None):\n",
    "    return tf.Variable(tf.constant(0.1, shape=[length]), name=name)\n",
    "\n",
    "\n",
    "def embedding_layer(input_x, vocabulary_size, embedding_size):\n",
    "    init_embeds = tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    "    embeddings = tf.Variable(init_embeds)\n",
    "    layer = tf.nn.embedding_lookup(embeddings, input_x)\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 750, Validation: 125, Test: 125\n",
      "if you have n't gone here go now\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Load all data\n",
    "X, Y = get_data(\"dataset\")\n",
    "assert(X.shape == Y.shape)\n",
    "\n",
    "# DEBUG PURPOSES\n",
    "X = X[:1000]\n",
    "Y = Y[:1000]\n",
    "\n",
    "\n",
    "\n",
    "# Set seed for randomness and shuffle dataset\n",
    "np.random.seed(42)\n",
    "index_shuf = list(range(len(X)))\n",
    "np.random.shuffle(index_shuf)\n",
    "\n",
    "X = X[index_shuf]\n",
    "Y = Y[index_shuf]\n",
    "\n",
    "# Calculate indeces to split\n",
    "split_train = 0.75\n",
    "train_size = int(split_train * len(X))\n",
    "val_size = int((round(1 - split_train, 3) / 2) * len(X))\n",
    "\n",
    "# Split dataset\n",
    "X_train, Y_train = X[:train_size], Y[:train_size]\n",
    "X_val, Y_val = X[train_size:(train_size + val_size)], Y[train_size:(train_size + val_size)]\n",
    "X_test, Y_test = X[(train_size + val_size):], Y[(train_size + val_size):]\n",
    "\n",
    "print(\"Train: \" + str(X_train.shape[0]) + \", Validation: \" + str(X_val.shape[0]) + \", Test: \" + str(X_test.shape[0]))\n",
    "print(\" \".join(X_train[0]))\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 32)\n",
      "(125, 32)\n",
      "(125, 32)\n"
     ]
    }
   ],
   "source": [
    "max_length = max_length_sentence(X_train)\n",
    "\n",
    "# Build dictionary\n",
    "vocab = dictionary.LanguageDictionary(X_train, max_length)\n",
    "\n",
    "# Transform word to indices\n",
    "X_train_indices = np.array([pad_sentence(vocab.text_to_indices(tmp), max_length, padding_value=0) for tmp in X_train])\n",
    "X_val_indices = np.array([pad_sentence(vocab.text_to_indices(tmp), max_length, padding_value=0) for tmp in X_val])\n",
    "X_test_indices = np.array([pad_sentence(vocab.text_to_indices(tmp), max_length, padding_value=0) for tmp in X_test])\n",
    "\n",
    "\n",
    "# Shapes\n",
    "print(X_train_indices.shape)\n",
    "print(X_val_indices.shape)\n",
    "print(X_test_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful variables\n",
    "timesteps = X_train_indices.shape[1]\n",
    "vocabulary_size = len(vocab.index_to_word)\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "embedding_size = 50\n",
    "hidden_units = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' If I wanted to use only \"Unidirectional LSTM\"\\n\\ncell = tf.contrib.rnn.LSTMCell(num_units=hidden_units, state_is_tuple=True)\\noutputs, last_states = tf.nn.dynamic_rnn(cell=cell, dtype=tf.float32, inputs=embedding)\\noutputs = tf.transpose(outputs, [1, 0, 2])\\nlast_output = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "inputs = tf.placeholder(tf.int32, (None, timesteps), 'inputs')\n",
    "labels = tf.placeholder(tf.int32, (None), 'output')\n",
    "input_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_input')\n",
    "output_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_output')\n",
    "\n",
    "# Embedding layer => Output shape is [batch_size, timesteps, embedding_size]\n",
    "embedding = embedding_layer(inputs, vocabulary_size, embedding_size)\n",
    "\n",
    "'''\n",
    "    Bidirectional LSTM\n",
    "'''\n",
    "# Forward direction cell\n",
    "lstm_fw_cell = tf.contrib.rnn.LSTMCell(hidden_units, forget_bias=1.0)\n",
    "\n",
    "# Backward direction cell\n",
    "lstm_bw_cell = tf.contrib.rnn.LSTMCell(hidden_units, forget_bias=1.0)\n",
    "\n",
    "# Input shape of any RNN should be [batch_size, embedding_size]\n",
    "outputs, last_states = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, embedding, dtype=tf.float32)\n",
    "\n",
    "# Unpack forward and backward outputs\n",
    "outputs_fw, outputs_bw = outputs[0], outputs[1]\n",
    "\n",
    "# This is a MANY-to-ONE model (sequence classification) => I only take output from last timestamp\n",
    "outputs_fw = tf.transpose(outputs_fw, [1, 0, 2])\n",
    "last_output_fw = tf.gather(outputs_fw, int(outputs_fw.get_shape()[0]) - 1)\n",
    "\n",
    "# Get last output of backward LSTM\n",
    "outputs_bw = tf.transpose(outputs_bw, [1, 0, 2])\n",
    "last_output_bw = tf.gather(outputs_bw, int(outputs_bw.get_shape()[0]) - 1)\n",
    "\n",
    "# Concat outputs\n",
    "outputs_concat = tf.concat([last_output_fw, last_output_bw], 1) \n",
    "logits = tf.layers.dense(inputs=outputs_concat, units=2, activation=None)\n",
    "\n",
    "\n",
    "''' If I wanted to use only \"Unidirectional LSTM\"\n",
    "\n",
    "cell = tf.contrib.rnn.LSTMCell(num_units=hidden_units, state_is_tuple=True)\n",
    "outputs, last_states = tf.nn.dynamic_rnn(cell=cell, dtype=tf.float32, inputs=embedding)\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "last_output = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"inputs:0\", shape=(?, 32), dtype=int32)\n",
      "Tensor(\"embedding_lookup/Identity:0\", shape=(?, 32, 50), dtype=float32)\n",
      "Tensor(\"transpose:0\", shape=(32, ?, 64), dtype=float32)\n",
      "Tensor(\"transpose_1:0\", shape=(32, ?, 64), dtype=float32)\n",
      "Tensor(\"GatherV2:0\", shape=(?, 64), dtype=float32)\n",
      "Tensor(\"concat:0\", shape=(?, 128), dtype=float32)\n",
      "Tensor(\"dense/BiasAdd:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "print(embedding)\n",
    "print(outputs_fw)\n",
    "print(outputs_bw)\n",
    "print(last_output_fw)\n",
    "print(outputs_concat)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Cross entropy loss after softmax of logits\n",
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "loss = tf.reduce_mean(ce)\n",
    "\n",
    "# Optimizer for gradients\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "# Useful tensors\n",
    "scores = tf.nn.softmax(logits)\n",
    "predictions = tf.to_int32(tf.argmax(scores, axis=1))\n",
    "correct_mask = tf.to_float(tf.equal(predictions, labels))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num iterations training 11\n",
      "Accuracy: 0.515625, Loss: 0.69495094\n",
      "VALIDATION loss: 0.69567144, accuracy: 0.515625\n",
      "Accuracy: 0.59375, Loss: 0.6922639\n",
      "Accuracy: 0.5, Loss: 0.6878749\n",
      "Accuracy: 0.453125, Loss: 0.69198775\n",
      "Accuracy: 0.578125, Loss: 0.6850026\n",
      "Accuracy: 0.640625, Loss: 0.6478636\n",
      "Accuracy: 0.59375, Loss: 0.6514138\n",
      "VALIDATION loss: 0.6974224, accuracy: 0.53125\n",
      "Accuracy: 0.75, Loss: 0.5396397\n",
      "VALIDATION loss: 0.65720737, accuracy: 0.640625\n"
     ]
    }
   ],
   "source": [
    "num_iterations_training = max(len(X_train_indices) // batch_size, 1)\n",
    "print(\"Num iterations training \" + str(num_iterations_training))\n",
    "\n",
    "# Initializer for variables in the graph\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Before each epoch, shuffle training dataset\n",
    "indices = list(range(len(X_train_indices)))\n",
    "\n",
    "# Validation data variables\n",
    "max_val_acc = 0\n",
    "val_batch_size = 64\n",
    "num_iterations_validation = max(len(X_val_indices) // val_batch_size, 1)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(epochs):  \n",
    "        \n",
    "        # Shuffle indices with a random seed\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        X_train_indices = X_train_indices[indices]\n",
    "        Y_train = Y_train[indices]\n",
    "\n",
    "        for j in range(num_iterations_training):\n",
    "            start_index = j * batch_size\n",
    "            end_index = (j + 1) * batch_size\n",
    "\n",
    "            # Forward and backpropagation on training data\n",
    "            _, train_loss, train_acc = sess.run([optimizer, loss, accuracy], feed_dict={\n",
    "                                                            inputs : X_train_indices[start_index:end_index],\n",
    "                                                            labels : Y_train[start_index:end_index]})\n",
    "            \n",
    "            # Print training loss and accuracy\n",
    "            if j % 30 == 0:\n",
    "                print(\"Accuracy: \" + str(train_acc) + \", Loss: \" + str(train_loss))\n",
    "                \n",
    "                \n",
    "            # Check accuracy on validation \n",
    "            if j % 30 == 0:\n",
    "                \n",
    "                # Accumulate loss and accuracy\n",
    "                val_loss_arr, val_acc_arr = [], []\n",
    "                \n",
    "                # Iterate over validation mini-batches\n",
    "                for k in range(num_iterations_validation):\n",
    "                    start_index_val = k * val_batch_size\n",
    "                    end_index_val = (k + 1) * val_batch_size\n",
    "                    \n",
    "                    val_loss, val_acc = sess.run([loss, accuracy], feed_dict={\n",
    "                                                            inputs : X_val_indices[start_index:end_index],\n",
    "                                                            labels : Y_val[start_index:end_index]})\n",
    "                    val_loss_arr.append(val_loss)\n",
    "                    val_acc_arr.append(val_acc)\n",
    "\n",
    "                val_acc = np.mean(val_acc_arr)\n",
    "\n",
    "                # Save model if validation accuracy better\n",
    "                if val_acc > max_val_acc:\n",
    "                    max_val_acc = val_acc\n",
    "                    print(\"VALIDATION loss: \" + str(np.mean(val_loss_arr)) + \", accuracy: \" + str(val_acc))\n",
    "                    save_path = saver.save(sess, \"./checkpoints/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
