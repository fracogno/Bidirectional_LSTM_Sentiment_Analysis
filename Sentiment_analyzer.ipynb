{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import dictionary\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Text processing functions\n",
    "'''\n",
    "def preprocess_sentence(sentence):\n",
    "    # Transform some punctuation to space\n",
    "    line = re.sub(r\"[,.;@#?!]+\\ *\", \" \", sentence)\n",
    "    \n",
    "    # Convert to lower case\n",
    "    line = line.lower()\n",
    "\n",
    "    # Tokenize words\n",
    "    default_wt = nltk.word_tokenize\n",
    "    line = default_wt(line)\n",
    "    \n",
    "    return line\n",
    "\n",
    "\n",
    "def max_length_sentence(dataset):\n",
    "    return max([len(line) for line in dataset])\n",
    "\n",
    "\n",
    "def pad_sentence(tokenized_sentence, max_length_sentence, padding_value=0):\n",
    "    \n",
    "    pad_length = max_length_sentence - len(tokenized_sentence)\n",
    "    sentence = list(tokenized_sentence)\n",
    "    \n",
    "    if pad_length > 0:\n",
    "        return np.pad(tokenized_sentence, (0, pad_length), mode='constant', constant_values=int(padding_value))\n",
    "    else:\n",
    "        return sentence[:max_length_sentence]\n",
    "\n",
    "\n",
    "# Dataset format: \"sentence \\t score \\n\"\n",
    "# Score is either 1 (for positive) or 0 (for negative)\n",
    "def get_data(directory):\n",
    "    X, Y = [], []\n",
    "\n",
    "    # Iterate over fils names in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "\n",
    "        if not filename.startswith('.'):            \n",
    "            with open(str(directory + '/' + filename)) as file:\n",
    "                \n",
    "                for line in file:\n",
    "                    splitted = line.split(\"\\t\")\n",
    "                    X.append(preprocess_sentence(splitted[0]))\n",
    "                    Y.append(int(splitted[1].split(\"\\n\")[0]))\n",
    "        \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "    Neural Network functions\n",
    "'''\n",
    "def new_weights(shape, name=None):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=name)\n",
    "\n",
    "\n",
    "def new_biases(length, name=None):\n",
    "    return tf.Variable(tf.constant(0.1, shape=[length]), name=name)\n",
    "\n",
    "\n",
    "def embedding_layer(input_x, vocabulary_size, embedding_size):\n",
    "    init_embeds = tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    "    embeddings = tf.Variable(init_embeds)\n",
    "    layer = tf.nn.embedding_lookup(embeddings, input_x)\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1500, Validation: 250, Test: 250\n",
      "last time buying from you\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Load all data\n",
    "X, Y = get_data(\"dataset\")\n",
    "assert(X.shape == Y.shape)\n",
    "\n",
    "#DEBUG\n",
    "X = X[:2000]\n",
    "Y = Y[:2000]\n",
    "\n",
    "# Set seed for randomness and shuffle dataset\n",
    "np.random.seed(42)\n",
    "index_shuf = list(range(len(X)))\n",
    "np.random.shuffle(index_shuf)\n",
    "\n",
    "X = X[index_shuf]\n",
    "Y = Y[index_shuf]\n",
    "\n",
    "# Calculate indeces to split\n",
    "split_train = 0.75\n",
    "train_size = int(split_train * len(X))\n",
    "val_size = int((round(1 - split_train, 3) / 2) * len(X))\n",
    "\n",
    "# Split dataset\n",
    "X_train, Y_train = X[:train_size], Y[:train_size]\n",
    "X_val, Y_val = X[train_size:(train_size + val_size)], Y[train_size:(train_size + val_size)]\n",
    "X_test, Y_test = X[(train_size + val_size):], Y[(train_size + val_size):]\n",
    "\n",
    "print(\"Train: \" + str(X_train.shape[0]) + \", Validation: \" + str(X_val.shape[0]) + \", Test: \" + str(X_test.shape[0]))\n",
    "print(\" \".join(X_train[0]))\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 33)\n",
      "(250, 33)\n",
      "(250, 33)\n"
     ]
    }
   ],
   "source": [
    "max_length = max_length_sentence(X_train)\n",
    "\n",
    "# Build dictionary\n",
    "vocab = dictionary.LanguageDictionary(X_train, max_length)\n",
    "\n",
    "# Transform word to indices\n",
    "X_train_indices = np.array([pad_sentence(vocab.text_to_indices(tmp), max_length, padding_value=0) for tmp in X_train])\n",
    "X_val_indices = np.array([pad_sentence(vocab.text_to_indices(tmp), max_length, padding_value=0) for tmp in X_val])\n",
    "X_test_indices = np.array([pad_sentence(vocab.text_to_indices(tmp), max_length, padding_value=0) for tmp in X_test])\n",
    "\n",
    "\n",
    "# Shapes\n",
    "print(X_train_indices.shape)\n",
    "print(X_val_indices.shape)\n",
    "print(X_test_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful variables\n",
    "timesteps = X_train_indices.shape[1]\n",
    "vocabulary_size = len(vocab.index_to_word)\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "embedding_size = 50\n",
    "hidden_units = 64\n",
    "dropout_prob = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' !!! If single layer bidirectional LSTM, without dropout !!!\\n\\nlstm_fw_cell = tf.contrib.rnn.LSTMCell(hidden_units, forget_bias=1.0)\\nlstm_bw_cell = tf.contrib.rnn.LSTMCell(hidden_units, forget_bias=1.0)\\noutputs, last_states = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, embedding, dtype=tf.float32)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "inputs = tf.placeholder(tf.int32, (None, timesteps), 'inputs')\n",
    "labels = tf.placeholder(tf.int32, (None), 'output')\n",
    "input_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_input')\n",
    "output_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_output')\n",
    "\n",
    "# Embedding layer => Output shape is [batch_size, timesteps, embedding_size]\n",
    "embedding = embedding_layer(inputs, vocabulary_size, embedding_size)\n",
    "\n",
    "\n",
    "'''\n",
    "    Multilayered Bidirectional LSTM\n",
    "'''\n",
    "num_layers_lstm = 3\n",
    "lstm_layers_vector_fw = []\n",
    "lstm_layers_vector_bw = []\n",
    "for _ in range(num_layers_lstm):\n",
    "\n",
    "    # Forward and backward direction cell\n",
    "    lstm_fw_cell = tf.contrib.rnn.LSTMCell(hidden_units, forget_bias=1.0)\n",
    "    lstm_bw_cell = tf.contrib.rnn.LSTMCell(hidden_units, forget_bias=1.0)\n",
    "    \n",
    "    # Dropout to generalize better\n",
    "    dropout_fw = tf.contrib.rnn.DropoutWrapper(lstm_fw_cell, input_keep_prob=input_keep_prob,\n",
    "                                               output_keep_prob=output_keep_prob)\n",
    "    \n",
    "    dropout_bw = tf.contrib.rnn.DropoutWrapper(lstm_bw_cell, input_keep_prob=input_keep_prob,\n",
    "                                               output_keep_prob=output_keep_prob)\n",
    "    # Append layers\n",
    "    lstm_layers_vector_fw.append(dropout_fw)\n",
    "    lstm_layers_vector_bw.append(dropout_bw)\n",
    "\n",
    "# Multi RNN layer\n",
    "multi_fw_cells = tf.contrib.rnn.MultiRNNCell(lstm_layers_vector_fw, state_is_tuple=True)\n",
    "multi_bw_cells = tf.contrib.rnn.MultiRNNCell(lstm_layers_vector_bw, state_is_tuple=True)\n",
    "\n",
    "# Input shape of any RNN should be [batch_size, embedding_size] and unpack outputs for forward and backward\n",
    "(outputs_fw, outputs_bw), last_states = tf.nn.bidirectional_dynamic_rnn(multi_fw_cells, multi_bw_cells, embedding, dtype=tf.float32)\n",
    "\n",
    "# This is a MANY-to-ONE model (sequence classification) => I only take output from last timestamp\n",
    "outputs_fw = tf.transpose(outputs_fw, [1, 0, 2])\n",
    "last_output_fw = tf.gather(outputs_fw, int(outputs_fw.get_shape()[0]) - 1)\n",
    "\n",
    "# Get last output of backward LSTM\n",
    "outputs_bw = tf.transpose(outputs_bw, [1, 0, 2])\n",
    "last_output_bw = tf.gather(outputs_bw, int(outputs_bw.get_shape()[0]) - 1)\n",
    "\n",
    "# Concat outputs\n",
    "outputs_concat = tf.concat([last_output_fw, last_output_bw], 1) \n",
    "logits = tf.layers.dense(inputs=outputs_concat, units=2, activation=None)\n",
    "\n",
    "\n",
    "''' !!! If only Unidirectional LSTM !!!\n",
    "\n",
    "cell = tf.contrib.rnn.LSTMCell(num_units=hidden_units, state_is_tuple=True)\n",
    "outputs, last_states = tf.nn.dynamic_rnn(cell=cell, dtype=tf.float32, inputs=embedding)\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "last_output = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)'''\n",
    "\n",
    "\n",
    "''' !!! If single layer bidirectional LSTM, without dropout !!!\n",
    "\n",
    "lstm_fw_cell = tf.contrib.rnn.LSTMCell(hidden_units, forget_bias=1.0)\n",
    "lstm_bw_cell = tf.contrib.rnn.LSTMCell(hidden_units, forget_bias=1.0)\n",
    "outputs, last_states = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, embedding, dtype=tf.float32)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"inputs:0\", shape=(?, 33), dtype=int32)\n",
      "Tensor(\"embedding_lookup/Identity:0\", shape=(?, 33, 50), dtype=float32)\n",
      "Tensor(\"transpose:0\", shape=(33, ?, 64), dtype=float32)\n",
      "Tensor(\"transpose_1:0\", shape=(33, ?, 64), dtype=float32)\n",
      "Tensor(\"GatherV2:0\", shape=(?, 64), dtype=float32)\n",
      "Tensor(\"concat:0\", shape=(?, 128), dtype=float32)\n",
      "Tensor(\"dense/BiasAdd:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "print(embedding)\n",
    "print(outputs_fw)\n",
    "print(outputs_bw)\n",
    "print(last_output_fw)\n",
    "print(outputs_concat)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Cross entropy loss after softmax of logits\n",
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "loss = tf.reduce_mean(ce)\n",
    "\n",
    "# Optimizer for gradients\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "# Useful tensors\n",
    "scores = tf.nn.softmax(logits)\n",
    "predictions = tf.to_int32(tf.argmax(scores, axis=1))\n",
    "correct_mask = tf.to_float(tf.equal(predictions, labels))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num iterations training 23\n",
      "Accuracy: 0.53125, Loss: 0.6939914\n",
      "VALIDATION loss: 0.6949105, accuracy: 0.484375\n",
      "Accuracy: 0.4375, Loss: 0.7008573\n",
      "VALIDATION loss: 0.69032174, accuracy: 0.59375\n",
      "Accuracy: 0.5, Loss: 0.6857712\n",
      "Accuracy: 0.59375, Loss: 0.672788\n",
      "Accuracy: 0.65625, Loss: 0.63963395\n",
      "VALIDATION loss: 0.61222947, accuracy: 0.671875\n",
      "Accuracy: 0.796875, Loss: 0.56286347\n",
      "VALIDATION loss: 0.5867394, accuracy: 0.703125\n",
      "Accuracy: 0.75, Loss: 0.58552885\n",
      "VALIDATION loss: 0.54668367, accuracy: 0.75\n",
      "Accuracy: 0.703125, Loss: 0.5857817\n",
      "VALIDATION loss: 0.48187307, accuracy: 0.8125\n",
      "Accuracy: 0.75, Loss: 0.5323098\n",
      "Accuracy: 0.84375, Loss: 0.40069225\n",
      "Accuracy: 0.75, Loss: 0.62761736\n",
      "Accuracy: 0.84375, Loss: 0.41780436\n",
      "VALIDATION loss: 0.36396733, accuracy: 0.84375\n",
      "Accuracy: 0.84375, Loss: 0.36374125\n",
      "Accuracy: 0.9375, Loss: 0.21171078\n",
      "VALIDATION loss: 0.37525117, accuracy: 0.859375\n",
      "Accuracy: 0.875, Loss: 0.31901953\n",
      "VALIDATION loss: 0.32872903, accuracy: 0.890625\n",
      "Accuracy: 0.921875, Loss: 0.17749065\n",
      "Accuracy: 0.828125, Loss: 0.3729307\n",
      "Accuracy: 0.890625, Loss: 0.3107929\n",
      "VALIDATION loss: 0.2991742, accuracy: 0.90625\n",
      "Accuracy: 0.9375, Loss: 0.16482314\n",
      "Accuracy: 0.9375, Loss: 0.15546769\n",
      "Accuracy: 0.921875, Loss: 0.21568477\n",
      "Accuracy: 0.96875, Loss: 0.16830598\n",
      "Accuracy: 0.953125, Loss: 0.16323358\n",
      "Accuracy: 0.953125, Loss: 0.18624422\n",
      "Accuracy: 0.953125, Loss: 0.0740161\n",
      "Accuracy: 0.921875, Loss: 0.18186986\n",
      "VALIDATION loss: 0.26311877, accuracy: 0.921875\n",
      "Accuracy: 0.9375, Loss: 0.14300644\n",
      "Accuracy: 0.96875, Loss: 0.12284444\n",
      "Accuracy: 0.96875, Loss: 0.0923643\n",
      "Accuracy: 0.9375, Loss: 0.16752586\n",
      "Accuracy: 0.953125, Loss: 0.2066874\n",
      "Accuracy: 0.921875, Loss: 0.16641183\n",
      "Accuracy: 0.953125, Loss: 0.1654924\n",
      "Accuracy: 0.984375, Loss: 0.048153058\n",
      "Accuracy: 0.984375, Loss: 0.050161354\n",
      "Accuracy: 0.984375, Loss: 0.044559713\n",
      "Accuracy: 0.984375, Loss: 0.045179393\n",
      "Accuracy: 0.921875, Loss: 0.20951521\n",
      "Accuracy: 0.984375, Loss: 0.058326855\n",
      "Accuracy: 0.96875, Loss: 0.123859435\n",
      "Accuracy: 0.9375, Loss: 0.101651594\n",
      "Accuracy: 1.0, Loss: 0.016036814\n",
      "Accuracy: 0.953125, Loss: 0.123116635\n",
      "Accuracy: 1.0, Loss: 0.025734346\n",
      "Accuracy: 0.96875, Loss: 0.11943813\n",
      "Accuracy: 0.984375, Loss: 0.07950932\n",
      "Accuracy: 0.953125, Loss: 0.119325995\n",
      "Accuracy: 0.953125, Loss: 0.06286515\n",
      "Accuracy: 0.984375, Loss: 0.027070541\n",
      "Accuracy: 1.0, Loss: 0.012648664\n"
     ]
    }
   ],
   "source": [
    "num_iterations_training = max(len(X_train_indices) // batch_size, 1)\n",
    "print(\"Num iterations training \" + str(num_iterations_training))\n",
    "\n",
    "# Initializer for variables in the graph\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Before each epoch, shuffle training dataset\n",
    "indices = list(range(len(X_train_indices)))\n",
    "\n",
    "# Validation data variables\n",
    "max_val_acc = 0\n",
    "num_iterations_validation = max(len(X_val_indices) // batch_size, 1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(epochs):  \n",
    "        \n",
    "        # Shuffle indices with a random seed\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        X_train_indices = X_train_indices[indices]\n",
    "        Y_train = Y_train[indices]\n",
    "\n",
    "        for j in range(num_iterations_training):\n",
    "            start_index = j * batch_size\n",
    "            end_index = (j + 1) * batch_size\n",
    "\n",
    "            # Forward and backpropagation on training data\n",
    "            _, train_loss, train_acc = sess.run([optimizer, loss, accuracy], feed_dict={\n",
    "                                                            inputs : X_train_indices[start_index:end_index],\n",
    "                                                            labels : Y_train[start_index:end_index],\n",
    "                                                            input_keep_prob : dropout_prob,\n",
    "                                                            output_keep_prob : dropout_prob})\n",
    "            \n",
    "            # Print training loss and accuracy\n",
    "            if j % 30 == 0:\n",
    "                print(\"Accuracy: \" + str(train_acc) + \", Loss: \" + str(train_loss))\n",
    "                \n",
    "                \n",
    "            # Check accuracy on validation \n",
    "            if j % 30 == 0:\n",
    "                \n",
    "                # Accumulate loss and accuracy\n",
    "                val_loss_arr, val_acc_arr = [], []\n",
    "                \n",
    "                # Iterate over validation mini-batches\n",
    "                for k in range(num_iterations_validation):\n",
    "                    start_index_val = k * batch_size\n",
    "                    end_index_val = (k + 1) * batch_size\n",
    "                    \n",
    "                    val_loss, val_acc = sess.run([loss, accuracy], feed_dict={\n",
    "                                                            inputs : X_val_indices[start_index:end_index],\n",
    "                                                            labels : Y_val[start_index:end_index],\n",
    "                                                            input_keep_prob : 1.0,\n",
    "                                                            output_keep_prob : 1.0})\n",
    "                    val_loss_arr.append(val_loss)\n",
    "                    val_acc_arr.append(val_acc)\n",
    "\n",
    "                val_acc = np.mean(val_acc_arr)\n",
    "\n",
    "                # Save model if validation accuracy better\n",
    "                if val_acc > max_val_acc:\n",
    "                    max_val_acc = val_acc\n",
    "                    print(\"VALIDATION loss: \" + str(np.mean(val_loss_arr)) + \", accuracy: \" + str(val_acc))\n",
    "                    save_path = saver.save(sess, \"./checkpoints/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/model.ckpt\n",
      "[array([   9,   26,  105, 1111,    3,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])]\n",
      "POSITIVE 0.9644934\n"
     ]
    }
   ],
   "source": [
    "sentence = \"It is an incredible structure\"\n",
    "sentence = [preprocess_sentence(sentence)]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, \"./checkpoints/model.ckpt\") \n",
    "    \n",
    "    indices_sentence = [pad_sentence(vocab.text_to_indices(tmp), max_length, padding_value=0) for tmp in sentence]\n",
    "    print(indices_sentence)\n",
    "    \n",
    "    score, pred = sess.run([scores, predictions], feed_dict={ inputs : indices_sentence,\n",
    "                                                            input_keep_prob : 1.0,\n",
    "                                                            output_keep_prob : 1.0 })\n",
    "    \n",
    "    if pred[0] == 1:\n",
    "        print(\"POSITIVE \" + str(score[0][1]))\n",
    "    else:\n",
    "        print(\"NEGATIVE \" + str(score[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
